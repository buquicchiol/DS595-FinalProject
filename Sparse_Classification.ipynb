{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "import warnings\n",
    "import gensim \n",
    "import h5py\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "warnings.filterwarnings(action='ignore') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv(\"./Data/train.csv\")\n",
    "test_data_ = pd.read_csv(\"./Data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve a list of the questions in the training set\n",
    "train_question1s = train_data[\"question1\"].astype(str).tolist()\n",
    "train_question2s = train_data[\"question2\"].astype(str).tolist()\n",
    "\n",
    "# Combine the two for vectorization and dimension reduction\n",
    "train_questions = train_question1s + train_question2s\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "data = []\n",
    "\n",
    "# Make each question a list of cleaned words\n",
    "for q in train_questions:\n",
    "    # Remove punctuation\n",
    "    q = q.translate(translator)\n",
    "    # Make everything lowercase\n",
    "    q = q.lower()\n",
    "    # Append it back to the list\n",
    "    data.append(q.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150383994, 1916334600)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare and train the Word2Vec model\n",
    "# Note: Min_count is the minimum number of times a word needs to appear for the model to consider it.\n",
    "#       If this is changed from 1, what we are doing will not work.\n",
    "model = gensim.models.Word2Vec(data, size=256, window=10, min_count=1, workers=25)\n",
    "model.train(data, total_examples=len(data), epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word2vec model\n",
    "with open(\"./word2vec_model.p\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n5\n14\n211\n241\n320\n1077\n1614\n20875\n33209\n35101\n408616\n422345\n422345\n"
     ]
    }
   ],
   "source": [
    "# Find the max number of words in all questions \n",
    "data = data\n",
    "\n",
    "_max = 0\n",
    "idx = 0\n",
    "i = 0\n",
    "for q in data:\n",
    "    if len(q) > _max:\n",
    "        idx = i\n",
    "        _max = len(q)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad every question to be the same length\n",
    "for q in data:\n",
    "    if len(q) < _max:\n",
    "        q += ([''] * (_max - len(q)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate the \"tokenized\" questions into original two lists\n",
    "q1_data = data[:len(train_question1s)]\n",
    "q2_data = data[len(train_question2s):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the similarity scores between each vectorized question \n",
    "similarity_scores = []\n",
    "\n",
    "for i in range(len(q1_data)):\n",
    "    pairwise_similarity = []\n",
    "    \n",
    "    for j in range(_max):\n",
    "        \n",
    "        q1_word = model[q1_data[i][j]].reshape((1, 256))\n",
    "        q2_sent = model[q2_data[i]]\n",
    "        \n",
    "        c_sim = cosine_similarity(q1_word, q2_sent, dense_output=False)\n",
    "        pairwise_similarity.append(c_sim.tolist())\n",
    "\n",
    "    similarity_scores.append(pairwise_similarity)\n",
    "\n",
    "similarity_scores = np.array(similarity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the similarity scores\n",
    "with h5py.File('similarity_scores.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"similarity-scores\",  data=similarity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the similarity scores. This can be skipped if you are running the whole way though\n",
    "with h5py.File('similarity_scores.h5', 'r') as hf:\n",
    "    similarity_scores = hf['similarity-scores'][:]\n",
    "\n",
    "# Show a random image to demonstrate sparsity and where the actual data lives\n",
    "sample_image = similarity_scores[np.random.randint(0, 29999)]\n",
    "plt.imshow(sample_image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the similarity scores to a preserve the original data\n",
    "#    This allows us to play with how sparse we want to make the results\n",
    "sparse_results = copy.copy(similarity_scores)\n",
    "sparse_results[sparse_results < .3] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the majority of our data lives in the upper-left quadrant\n",
    "#    Let's readjust the size to speed things up\n",
    "sub_sparse = sparse_results[:, :20, :20]\n",
    "\n",
    "# Make a random matrix to compress our data\n",
    "A = np.random.normal(loc=0, scale=1, size=(121, 20*20))\n",
    "\n",
    "comp_sub_sparse_data = []\n",
    "\n",
    "for i in range(sub_sparse.shape[0]):\n",
    "    # Bang every entry in our sub-sampled sparse dataset into this vector\n",
    "    comp_vec = np.matmul(A, sub_sparse[i].reshape(20*20, 1))\n",
    "    comp_sub_sparse_data.append(comp_vec)\n",
    "    \n",
    "# Convert this new data into a numpy array for convenience \n",
    "comp_sub_sparse_data = np.array(comp_sub_sparse_data)\n",
    "comp_sub_sparse_data = comp_sub_sparse_data.reshape((comp_sub_sparse_data.shape[0], comp_sub_sparse_data.shape[1]))\n",
    "print(comp_sub_sparse_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Because we only used the first 30,000 questions get their labels\n",
    "labels = train_data.loc[:29999]['is_duplicate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into a train-test split\n",
    "model_results = {}\n",
    "X_train, X_test, y_train, y_test = train_test_split(comp_sub_data, labels, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.1, loss='deviance', max_depth=10,\n              max_features='log2', max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=500,\n              n_iter_no_change=None, presort='auto', random_state=None,\n              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a generic XGBoost classifier\n",
    "xgb_classifier_full = GradientBoostingClassifier(n_estimators=500, max_features='log2', max_depth=10)\n",
    "xgb_classifier_full.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6953535353535354\n0.7616970582103109\n10.522203512269707\n"
     ]
    }
   ],
   "source": [
    "# Score it and save the results\n",
    "y_hat_proba_xgb = xgb_classifier_full.predict_proba(X_test)\n",
    "y_hat_xgb = xgb_classifier_full.predict(X_test)\n",
    "\n",
    "xgb_acc = accuracy_score(y_test, y_hat_xgb)\n",
    "xgb_roc_auc = roc_auc_score(y_test, y_hat_proba_xgb[:, 1])\n",
    "xgb_log_loss = log_loss(y_test, y_hat_xgb)\n",
    "\n",
    "model_results['XGBoost_full'] = [xgb_acc, xgb_roc_auc, xgb_log_loss]\n",
    "\n",
    "print(xgb_acc)\n",
    "print(xgb_roc_auc)\n",
    "print(xgb_log_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=10, max_features='sqrt', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a generic Random Forest Classifier\n",
    "rf_classifier_full = RandomForestClassifier(n_estimators=500, max_depth=10, max_features='sqrt', n_jobs=-1)\n",
    "rf_classifier_full.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6731313131313131\n0.7398514521097166\n11.289708777119738\n"
     ]
    }
   ],
   "source": [
    "# Score it and save the results\n",
    "y_hat_proba_rf = rf_classifier_full.predict_proba(X_test)\n",
    "y_hat_rf = rf_classifier_full.predict(X_test)\n",
    "\n",
    "rf_acc = accuracy_score(y_test, y_hat_rf)\n",
    "rf_roc_auc = roc_auc_score(y_test, y_hat_proba_rf[:, 1])\n",
    "rf_log_loss = log_loss(y_test, y_hat_rf)\n",
    "\n",
    "model_results['RandomForest_full'] = [rf_acc, rf_roc_auc, rf_log_loss]\n",
    "\n",
    "print(rf_acc)\n",
    "print(rf_roc_auc)\n",
    "print(rf_log_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n               store_covariance=False, store_covariances=None, tol=0.0001)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a generic QDA model\n",
    "qda_classifier_full = QuadraticDiscriminantAnalysis()\n",
    "qda_classifier_full.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5667676767676768\n0.6845759605439403\n14.963593552130112\n"
     ]
    }
   ],
   "source": [
    "# Score it and save the results\n",
    "y_hat_proba_qda = qda_classifier_full.predict_proba(X_test)\n",
    "y_hat_qda = qda_classifier_full.predict(X_test)\n",
    "\n",
    "qda_acc = accuracy_score(y_test, y_hat_qda)\n",
    "qda_roc_auc = roc_auc_score(y_test, y_hat_proba_qda[:, 1])\n",
    "qda_log_loss = log_loss(y_test, y_hat_qda)\n",
    "\n",
    "model_results['QDA_full'] = [qda_acc, qda_roc_auc, qda_log_loss]\n",
    "\n",
    "print(qda_acc)\n",
    "print(qda_roc_auc)\n",
    "print(qda_log_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With L2 Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with some normalized data\n",
    "# Normalize the Data with L2\n",
    "X_train_l2 = normalize(X_train, norm='l2')\n",
    "X_test_l2 = normalize(X_test, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.1, loss='deviance', max_depth=10,\n              max_features='log2', max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=500,\n              n_iter_no_change=None, presort='auto', random_state=None,\n              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try the same XGBoost model from above with the L2-normed data\n",
    "xgb_classifier_l2 = GradientBoostingClassifier(n_estimators=500, max_features='log2', max_depth=10)\n",
    "xgb_classifier_l2.fit(X_train_l2, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6864646464646464\n0.7444946332817566\n10.82920548898185\n"
     ]
    }
   ],
   "source": [
    "# Score it and save the results\n",
    "y_hat_proba_xgb_l2 = xgb_classifier_l2.predict_proba(X_test_l2)\n",
    "y_hat_xgb_l2 = xgb_classifier_l2.predict(X_test_l2)\n",
    "\n",
    "xgb_acc_l2 = accuracy_score(y_test, y_hat_xgb_l2)\n",
    "xgb_roc_auc_l2 = roc_auc_score(y_test, y_hat_proba_xgb_l2[:, 1])\n",
    "xgb_log_loss_l2 = log_loss(y_test, y_hat_xgb_l2)\n",
    "\n",
    "model_results['XGBoost_l2'] = [xgb_acc_l2, xgb_roc_auc_l2, xgb_log_loss_l2]\n",
    "\n",
    "print(xgb_acc_l2)\n",
    "print(xgb_roc_auc_l2)\n",
    "print(xgb_log_loss_l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Data with L1\n",
    "X_train_l1 = normalize(X_train, norm='l1')\n",
    "X_test_l1 = normalize(X_test, norm='l1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.1, loss='deviance', max_depth=10,\n              max_features='log2', max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=500,\n              n_iter_no_change=None, presort='auto', random_state=None,\n              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try the same XGBoost model from above with the L1-normed data\n",
    "xgb_classifier_l1 = GradientBoostingClassifier(n_estimators=500, max_features='log2', max_depth=10)\n",
    "xgb_classifier_l1.fit(X_train_l1, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6846464646464646\n0.7383700754536705\n10.892004314221747\n"
     ]
    }
   ],
   "source": [
    "# Score it and save the results\n",
    "y_hat_proba_xgb_l1 = xgb_classifier_l1.predict_proba(X_test_l1)\n",
    "y_hat_xgb_l1 = xgb_classifier_l1.predict(X_test_l1)\n",
    "\n",
    "xgb_acc_l1 = accuracy_score(y_test, y_hat_xgb_l1)\n",
    "xgb_roc_auc_l1 = roc_auc_score(y_test, y_hat_proba_xgb_l1[:, 1])\n",
    "xgb_log_loss_l1 = log_loss(y_test, y_hat_xgb_l1)\n",
    "\n",
    "model_results['XGBoost_l1'] = [xgb_acc_l1, xgb_roc_auc_l1, xgb_log_loss_l1]\n",
    "\n",
    "print(xgb_acc_l1)\n",
    "print(xgb_roc_auc_l1)\n",
    "print(xgb_log_loss_l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Data with MAX\n",
    "X_train_max = normalize(X_train, norm='max')\n",
    "X_test_max = normalize(X_test, norm='max')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.1, loss='deviance', max_depth=10,\n              max_features='log2', max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=500,\n              n_iter_no_change=None, presort='auto', random_state=None,\n              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try the same XGBoost model from above with the MAX-normed data\n",
    "xgb_classifier_max = GradientBoostingClassifier(n_estimators=500, max_features='log2', max_depth=10)\n",
    "xgb_classifier_max.fit(X_train_max, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score it and save the results\n",
    "y_hat_proba_xgb_max = xgb_classifier_max.predict_proba(X_test_max)\n",
    "y_hat_xgb_max = xgb_classifier_max.predict(X_test_max)\n",
    "\n",
    "xgb_acc_max = accuracy_score(y_test, y_hat_xgb_max)\n",
    "xgb_roc_auc_max = roc_auc_score(y_test, y_hat_proba_xgb_max[:, 1])\n",
    "xgb_log_loss_max = log_loss(y_test, y_hat_xgb_max)\n",
    "\n",
    "model_results['XGBoost_max'] = [xgb_acc_max, xgb_roc_auc_max, xgb_log_loss_max]\n",
    "\n",
    "print(xgb_acc_max)\n",
    "print(xgb_roc_auc_max)\n",
    "print(xgb_log_loss_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the metrics from the dictionary\n",
    "accs = []\n",
    "aucs = []\n",
    "llss = []\n",
    "\n",
    "for mod in model_results.keys():\n",
    "    accs.append(model_results[mod][0])\n",
    "    aucs.append(model_results[mod][1])\n",
    "    llss.append(model_results[mod][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracies\n",
    "plt.bar(model_results.keys(), accs)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./Accuracies.png', dpi=700)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the AUC-ROC's\n",
    "plt.bar(model_results.keys(), aucs)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./ROC_AUC.png', dpi=700)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Log-loss since the Kaggle competition used it.\n",
    "plt.bar(model_results.keys(), llss)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./Log_Loss.png', dpi=700)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
